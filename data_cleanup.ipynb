{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Intrusion detection system (IDS)\n",
                "# Single file investigation\n",
                "---\n",
                "\n",
                "The **CSE-CIC-IDS2018** dataset needs to be cleaned before it can be consumed for training the dataset. In its original structure the dataset has ten separate csv files, every one of them has network traffic logged for a specific day of working.\n",
                "\n",
                "An individual investigation of the dataset using one file is created.\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Connect and load single CSV file to colabs for investigation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "\n",
                "home_path = os.path.dirname(os.path.abspath(''))\n",
                "\n",
                "data_path = 'data/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv'\n",
                "file_path = os.path.join(home_path, data_path)\n",
                "\n",
                "df = pd.read_csv(file_path)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/home/am520/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
                        "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "df.info()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 331125 entries, 0 to 331124\n",
                        "Data columns (total 80 columns):\n",
                        " #   Column             Non-Null Count   Dtype \n",
                        "---  ------             --------------   ----- \n",
                        " 0   Dst Port           331125 non-null  object\n",
                        " 1   Protocol           331125 non-null  object\n",
                        " 2   Timestamp          331125 non-null  object\n",
                        " 3   Flow Duration      331125 non-null  object\n",
                        " 4   Tot Fwd Pkts       331125 non-null  object\n",
                        " 5   Tot Bwd Pkts       331125 non-null  object\n",
                        " 6   TotLen Fwd Pkts    331125 non-null  object\n",
                        " 7   TotLen Bwd Pkts    331125 non-null  object\n",
                        " 8   Fwd Pkt Len Max    331125 non-null  object\n",
                        " 9   Fwd Pkt Len Min    331125 non-null  object\n",
                        " 10  Fwd Pkt Len Mean   331125 non-null  object\n",
                        " 11  Fwd Pkt Len Std    331125 non-null  object\n",
                        " 12  Bwd Pkt Len Max    331125 non-null  object\n",
                        " 13  Bwd Pkt Len Min    331125 non-null  object\n",
                        " 14  Bwd Pkt Len Mean   331125 non-null  object\n",
                        " 15  Bwd Pkt Len Std    331125 non-null  object\n",
                        " 16  Flow Byts/s        329291 non-null  object\n",
                        " 17  Flow Pkts/s        331125 non-null  object\n",
                        " 18  Flow IAT Mean      331125 non-null  object\n",
                        " 19  Flow IAT Std       331125 non-null  object\n",
                        " 20  Flow IAT Max       331125 non-null  object\n",
                        " 21  Flow IAT Min       331125 non-null  object\n",
                        " 22  Fwd IAT Tot        331125 non-null  object\n",
                        " 23  Fwd IAT Mean       331125 non-null  object\n",
                        " 24  Fwd IAT Std        331125 non-null  object\n",
                        " 25  Fwd IAT Max        331125 non-null  object\n",
                        " 26  Fwd IAT Min        331125 non-null  object\n",
                        " 27  Bwd IAT Tot        331125 non-null  object\n",
                        " 28  Bwd IAT Mean       331125 non-null  object\n",
                        " 29  Bwd IAT Std        331125 non-null  object\n",
                        " 30  Bwd IAT Max        331125 non-null  object\n",
                        " 31  Bwd IAT Min        331125 non-null  object\n",
                        " 32  Fwd PSH Flags      331125 non-null  object\n",
                        " 33  Bwd PSH Flags      331125 non-null  object\n",
                        " 34  Fwd URG Flags      331125 non-null  object\n",
                        " 35  Bwd URG Flags      331125 non-null  object\n",
                        " 36  Fwd Header Len     331125 non-null  object\n",
                        " 37  Bwd Header Len     331125 non-null  object\n",
                        " 38  Fwd Pkts/s         331125 non-null  object\n",
                        " 39  Bwd Pkts/s         331125 non-null  object\n",
                        " 40  Pkt Len Min        331125 non-null  object\n",
                        " 41  Pkt Len Max        331125 non-null  object\n",
                        " 42  Pkt Len Mean       331125 non-null  object\n",
                        " 43  Pkt Len Std        331125 non-null  object\n",
                        " 44  Pkt Len Var        331125 non-null  object\n",
                        " 45  FIN Flag Cnt       331125 non-null  object\n",
                        " 46  SYN Flag Cnt       331125 non-null  object\n",
                        " 47  RST Flag Cnt       331125 non-null  object\n",
                        " 48  PSH Flag Cnt       331125 non-null  object\n",
                        " 49  ACK Flag Cnt       331125 non-null  object\n",
                        " 50  URG Flag Cnt       331125 non-null  object\n",
                        " 51  CWE Flag Count     331125 non-null  object\n",
                        " 52  ECE Flag Cnt       331125 non-null  object\n",
                        " 53  Down/Up Ratio      331125 non-null  object\n",
                        " 54  Pkt Size Avg       331125 non-null  object\n",
                        " 55  Fwd Seg Size Avg   331125 non-null  object\n",
                        " 56  Bwd Seg Size Avg   331125 non-null  object\n",
                        " 57  Fwd Byts/b Avg     331125 non-null  object\n",
                        " 58  Fwd Pkts/b Avg     331125 non-null  object\n",
                        " 59  Fwd Blk Rate Avg   331125 non-null  object\n",
                        " 60  Bwd Byts/b Avg     331125 non-null  object\n",
                        " 61  Bwd Pkts/b Avg     331125 non-null  object\n",
                        " 62  Bwd Blk Rate Avg   331125 non-null  object\n",
                        " 63  Subflow Fwd Pkts   331125 non-null  object\n",
                        " 64  Subflow Fwd Byts   331125 non-null  object\n",
                        " 65  Subflow Bwd Pkts   331125 non-null  object\n",
                        " 66  Subflow Bwd Byts   331125 non-null  object\n",
                        " 67  Init Fwd Win Byts  331125 non-null  object\n",
                        " 68  Init Bwd Win Byts  331125 non-null  object\n",
                        " 69  Fwd Act Data Pkts  331125 non-null  object\n",
                        " 70  Fwd Seg Size Min   331125 non-null  object\n",
                        " 71  Active Mean        331125 non-null  object\n",
                        " 72  Active Std         331125 non-null  object\n",
                        " 73  Active Max         331125 non-null  object\n",
                        " 74  Active Min         331125 non-null  object\n",
                        " 75  Idle Mean          331125 non-null  object\n",
                        " 76  Idle Std           331125 non-null  object\n",
                        " 77  Idle Max           331125 non-null  object\n",
                        " 78  Idle Min           331125 non-null  object\n",
                        " 79  Label              331125 non-null  object\n",
                        "dtypes: object(80)\n",
                        "memory usage: 202.1+ MB\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "A manual inspection of the file shows that the title of the columns exist many times as duplicates within the file rows. This implies that each individual file was joined using many csv files that resulted in replication of the column titles in the creation of one file. To solve this problem, all rows containing the duplicate titles are removed. Additionally, the title names are changed to remove spaces and enforce a lowercase format for simpler retrieval of the columns."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "import re\n",
                "df = df.drop_duplicates(keep=False)\n",
                "df.reset_index(drop=True, inplace = True)\n",
                "\n",
                "column_name_regex = re.compile(r\"\\W\", re.IGNORECASE)\n",
                "df.columns = [column_name_regex.sub('_', c.lower()) for c in df.columns]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Pandas `read_csv()` method cannot parse “Infinity” values correctly as the Pandas will only accept infinity values in the “inf” format. As such, all incidents containing “infinity” are substituted with the string “inf”. We also change the types of the columns so as to load the dataset more efficiently."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "df = df.replace('Infinity', 'inf')\n",
                "#downcasting integer types\n",
                "df[['dst_port', 'protocol', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'pkt_len_min', 'pkt_len_max', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts', 'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts', 'fwd_seg_size_min', 'active_max', 'active_min', 'idle_max', 'idle_min']] = df[['dst_port', 'protocol', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'pkt_len_min', 'pkt_len_max', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts', 'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts', 'fwd_seg_size_min', 'active_max', 'active_min', 'idle_max', 'idle_min']].apply(pd.to_numeric, downcast = 'integer')\n",
                "\n",
                "#downcasting float types\n",
                "df[['fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean', 'flow_iat_std', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_pkts_s', 'bwd_pkts_s', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'pkt_size_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'active_mean', 'active_std', 'idle_mean', 'idle_std']] = df[['fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean', 'flow_iat_std', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_pkts_s', 'bwd_pkts_s', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'pkt_size_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'active_mean', 'active_std', 'idle_mean', 'idle_std']].apply(pd.to_numeric, downcast = 'float') \n",
                "\n",
                "df.dtypes #check that types have changed\n",
                "\n",
                "#df = df.apply(pd.to_numeric, errors='ignore') standard type change, loads at 200mb, by downcasting this is reduced to 75mb+ "
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "dst_port           int32\n",
                            "protocol            int8\n",
                            "timestamp         object\n",
                            "flow_duration      int32\n",
                            "tot_fwd_pkts       int16\n",
                            "                  ...   \n",
                            "idle_mean        float32\n",
                            "idle_std         float32\n",
                            "idle_max           int32\n",
                            "idle_min           int32\n",
                            "label             object\n",
                            "Length: 80, dtype: object"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We have finished looking at a single file next we move on to loading all the files and creating one combined dataset."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "import glob\n",
                "home_path = os.path.dirname(os.path.abspath(''))\n",
                "files = [file for file in glob.glob(home_path + \"/data/*.csv\", recursive=True)]\n",
                "print(files) # check all files are listed.\n",
                "\n",
                "# place all datasets into dataframes\n",
                "\n",
                "df = [pd.read_csv(f) for f in files]\n",
                "\n",
                "# #check they have same number of columns \n",
                "for d in df:\n",
                "    print(d.shape)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['/home/am520/MSc2021/data/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv', '/home/am520/MSc2021/data/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv']\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/tmp/ipykernel_1965/3408969233.py:9: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
                        "  df = [pd.read_csv(f) for f in files]\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(1048575, 80)\n",
                        "(1048575, 80)\n",
                        "(1048575, 80)\n",
                        "(7948748, 84)\n",
                        "(1048575, 80)\n",
                        "(331125, 80)\n",
                        "(1048575, 80)\n",
                        "(1048575, 80)\n",
                        "(1048575, 80)\n",
                        "(613104, 80)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Of note, the file Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv has four columns Src IP, Src Port, Flow ID and Dst IP which are not included in the other files. As these files are not needed, they are removed from the file. This can be seen in file four with shape (7948748, 84)."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "df[3] = df[3].drop(['Src IP', 'Src Port', 'Flow ID' ,'Dst IP'], axis=1)\n",
                "df[3].shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(7948748, 80)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Once the singulr example has been run, we run garbage collect to free up memory before working on all the files. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "import gc\n",
                "del df\n",
                "gc.collect()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "8"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# All files comined and cleaned"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "In short, the next stages are used to clean all the files in the dataset:\n",
                "\n",
                "1. Eliminate duplicate titles/headers of each column\n",
                "\n",
                "2. Replace Infinity values with inf\n",
                "\n",
                "3. Change column titles to lowercase and remove spaces.\n",
                "\n",
                "4. Change types of the variables to numeric\n",
                "\n",
                "5. Combine all files\n",
                "\n",
                "Of note, the file Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv has four columns Src IP, Src Port, Flow ID and Dst IP which are not included in the other files. As these files are not needed, they are removed from the file."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "import re \n",
                "import pandas as pd\n",
                "import os\n",
                "import glob\n",
                "import fastparquet\n",
                "import gc \n",
                "\n",
                "def remove_duplicate_headers(d):    \n",
                "    return d[~d['Timestamp'].str.contains('Timestamp', na=False)]\n",
                "\n",
                "def substitute_infinity(d):\n",
                "    return df.replace('Infinity', 'inf')\n",
                "\n",
                "def transform_headers(d):\n",
                "    column_name_regex = re.compile(r\"\\W\", re.IGNORECASE)\n",
                "    return [column_name_regex.sub('_', c.lower()) for c in df.columns]\n",
                "\n",
                "def transform_type(df):\n",
                "    #downcasting integer types\n",
                "    df[['dst_port', 'protocol', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'pkt_len_min', 'pkt_len_max', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts', 'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts', 'fwd_seg_size_min', 'active_max', 'active_min', 'idle_max', 'idle_min']] = df[['dst_port', 'protocol', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'pkt_len_min', 'pkt_len_max', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts', 'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts', 'fwd_seg_size_min', 'active_max', 'active_min', 'idle_max', 'idle_min']].apply(pd.to_numeric, downcast = 'integer')\n",
                "    #downcasting float types\n",
                "    df[['fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean', 'flow_iat_std', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_pkts_s', 'bwd_pkts_s', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'pkt_size_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'active_mean', 'active_std', 'idle_mean', 'idle_std']] = df[['fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean', 'flow_iat_std', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_pkts_s', 'bwd_pkts_s', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'pkt_size_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'active_mean', 'active_std', 'idle_mean', 'idle_std']].apply(pd.to_numeric, downcast = 'float') \n",
                "    return df\n",
                "\n",
                "def check_dir_exists():\n",
                "    out_path = os.path.join(data_path, processed)\n",
                "    if not os.path.exists(out_path):\n",
                "        os.mkdir(out_path)\n",
                "\n",
                "\n",
                "processed = 'pre-clean/'\n",
                "data_path = os.path.dirname(os.path.abspath('')) + \"/data/\"\n",
                "check_dir_exists()\n",
                "\n",
                "\n",
                "files={\n",
                "'Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv' : '28-02-2018.parquet',\n",
                " 'Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv' : '01-03-2018.parquet',\n",
                " 'Friday-02-03-2018_TrafficForML_CICFlowMeter.csv' : '02-03-2018.parquet',\n",
                " 'Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv' : '22-02-2018.parquet',\n",
                " 'Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv' : '15-02-2018.parquet',\n",
                " 'Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv' : '20-02-2018.parquet',\n",
                " 'Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv' : '21-02-2018.parquet',\n",
                " 'Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv' : '14-02-2018.parquet',\n",
                " 'Friday-16-02-2018_TrafficForML_CICFlowMeter.csv' : '16-02-2018.parquet',\n",
                " 'Friday-23-02-2018_TrafficForML_CICFlowMeter.csv' : '23-02-2018.parquet'\n",
                "}\n",
                "\n",
                "for i, out in files.items():\n",
                "    path = os.path.join(data_path, i)\n",
                "    out_path = os.path.join(data_path, processed, out)\n",
                "    df = pd.read_csv(path, dtype=str).drop(columns=['Flow ID', 'Src IP', 'Dst IP', 'Src Port'], errors='ignore')\n",
                "    df = remove_duplicate_headers(df)\n",
                "    df = substitute_infinity(df)\n",
                "    df.columns = transform_headers(df)\n",
                "    df = transform_type(df)\n",
                "    df.to_parquet(out_path, engine='fastparquet')\n",
                "\n",
                "del df\n",
                "gc.collect()\n",
                "\n",
                "#combine all files and create combined output file using optimal file format\n",
                "\n",
                "clean_data_path = os.path.dirname(os.path.abspath('')) + \"/data/pre-clean/\"\n",
                "output_data_path = os.path.dirname(os.path.abspath('')) + \"/data/combined/\"\n",
                "files = [file for file in glob.glob(clean_data_path + \"**/*.parquet\", recursive=True)]\n",
                "df = [pd.read_parquet(f) for f in files]\n",
                "\n",
                "# #check they have same number of columns \n",
                "# for d in df:\n",
                "#     print(d.shape)\n",
                "\n",
                "processed = 'combined/'\n",
                "check_dir_exists()\n",
                "\n",
                "\n",
                "df = pd.concat([d for d in df])\n",
                "df.reset_index(drop=True, inplace = True)\n",
                "df = transform_type(df)\n",
                "df.to_parquet(output_data_path+'combined1.parquet', engine='fastparquet')\n",
                "df.to_feather(output_data_path+'combined1.feather')\n",
                "df.to_pickle(output_data_path+'combined1.pickle')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "We created three different types of file saves to address the size of the combined file namely, parquet, pickle and feather. In the following section we load each of these files and test their memory usage. Based on the best result, we will use that type going forward. In this case, we are going to use pickle as it had the best performance in regards to RAM."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "import os, psutil\n",
                "import pandas as pd\n",
                "process = psutil.Process(os.getpid())\n",
                "output_data_path = os.path.dirname(os.path.abspath('')) + \"/data/combined/\"\n",
                "\n",
                "df = pd.read_feather(output_data_path+'combined1.feather')\n",
                "print(\"Feather\", process.memory_info().rss)\n",
                "print('RAM memory % used:', psutil.virtual_memory()[2])\n",
                "print(psutil.getloadavg())\n",
                "\n",
                "df = pd.read_pickle(output_data_path+'combined1.pickle')\n",
                "print(\"Pickle\", process.memory_info().rss)\n",
                "print('RAM memory % used:', psutil.virtual_memory()[2])\n",
                "print(psutil.getloadavg())\n",
                "\n",
                "df = pd.read_parquet(output_data_path+'combined1.parquet')\n",
                "print(\"Parquet\", process.memory_info().rss)\n",
                "print('RAM memory % used:', psutil.virtual_memory()[2])\n",
                "print(psutil.getloadavg())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Feather 15999307776\n",
                        "RAM memory % used: 51.1\n",
                        "(0.13, 0.14, 0.41)\n",
                        "Pickle 14645739520\n",
                        "RAM memory % used: 47.1\n",
                        "(0.44, 0.2, 0.42)\n",
                        "Parquet 16192724992\n",
                        "RAM memory % used: 51.1\n",
                        "(0.89, 0.3, 0.45)\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit"
        },
        "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}